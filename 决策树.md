# 决策树

![](<屏幕截图 2025-10-14 172343.png>)

## 树的组成

1. 根节点：第一个选择点
2. 非叶子节点与分支：中间过程
3. 叶子节点：最终的决策结果

## 如何切分特征———熵

1. 目标：通过一种衡量标准，来计算通过不同特征进行分类选择后的分类情况，找出来最好的那个当成根节点，以此类推
2. 特性：衡量数据集的不确定性
3. 公式：\(Ent(D) = -\sum_{i=1}^k p_i \cdot \log_2 p_i\)
4. 信息增益(ID3)：通过特征A划分后，数据集的不确定性减少了多少（越大表明帮助越大）————划分前的熵-划分后的条件熵
5. 实例：

![alt text](<屏幕截图 2025-10-14 193526-1.png>)

![alt text](<屏幕截图 2025-10-14 193549-1.png>)

6. 信息增益率(C4.5)：旨在解决特征选择时对取值较多的特征偏向性的问题————****信息增益/固有信息**————固有信息时衡量特征A划分数据集的**细碎程度**，可以给多取值特征加惩罚分
7. CART:使用GINI系数（Gini=1−∑pi^2）作衡量标准